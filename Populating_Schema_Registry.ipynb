{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c724a69-568c-4e63-81ac-464383d2c2af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS ds_training_1.default.schema_registry (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY,  -- Unique ID for each schema entry\n",
    "    catalog_name VARCHAR(255),  -- Database/catalog name\n",
    "    schema_name VARCHAR(255),   -- Schema name of the table\n",
    "    table_name VARCHAR(255),    -- Table name\n",
    "    schema_version INT,         -- Schema version number\n",
    "    created_by VARCHAR(255),    -- User who created the schema\n",
    "    modified_by VARCHAR(255),   -- User who modified the schema\n",
    "    modified_timestamp TIMESTAMP, -- Timestamp of modification\n",
    "    schema_json STRING,         -- Schema represented as a JSON string\n",
    "    change_type VARCHAR(50),    -- Type of change (ADD, MODIFY, DELETE)\n",
    "    column_name VARCHAR(255),   -- Affected column name during schema change\n",
    "    table_version INT,          -- Table version from DESCRIBE HISTORY\n",
    "    table_version_timestamp TIMESTAMP,  -- Timestamp when this table version was recorded\n",
    "    status VARCHAR(50),         -- 'Active' or 'Inactive'\n",
    "    check_timestamp TIMESTAMP,  -- Timestamp when schema was checked\n",
    "    schema_change_alert_status VARCHAR(50),   -- 'Alert Sent' or 'Alert Pending'\n",
    "    rollback_notification_status VARCHAR(50) -- 'Notified' or 'Not Yet Notified'\n",
    "\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747463ea-d8fb-4549-8deb-3493e27efe7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for input\n",
    "dbutils.widgets.text(\"catalog_name\", \"\", \"catalog_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99a171a-0fc4-49be-8651-1e3769a01fed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = dbutils.widgets.get(\"catalog_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "299cca40-45e9-4f47-b011-1e6bd8cda7fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper function to check if the table exists in schema_registry\n",
    "def check_schema_registry(catalog_name, schema_name, table_name):\n",
    "    query = f\"\"\"\n",
    "        SELECT schema_version, table_version \n",
    "        FROM {catalog_name}.default.schema_registry \n",
    "        WHERE catalog_name = '{catalog_name}' \n",
    "          AND schema_name = '{schema_name}' \n",
    "          AND table_name = '{table_name}'\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Helper function to get the latest table version and timestamp from DESCRIBE HISTORY\n",
    "def get_latest_table_version(catalog_name, schema_name, table_name):\n",
    "    describe_history = spark.sql(f\"DESCRIBE HISTORY {catalog_name}.{schema_name}.{table_name}\")\n",
    "    latest_version = describe_history.agg({\"version\": \"max\"}).collect()[0][0]\n",
    "    latest_timestamp = describe_history.filter(f\"version = {latest_version}\").select(\"timestamp\").collect()[0][0]\n",
    "    return latest_version, latest_timestamp\n",
    "\n",
    "# Helper function to fetch the table owner from information_schema.tables\n",
    "def get_table_owner(catalog_name, schema_name, table_name):\n",
    "    query = f\"\"\"\n",
    "        SELECT table_owner\n",
    "        FROM {catalog_name}.information_schema.tables \n",
    "        WHERE table_schema = '{schema_name}' \n",
    "          AND table_name = '{table_name}'\n",
    "    \"\"\"\n",
    "    result = spark.sql(query).collect()\n",
    "    return result[0]['table_owner'] if result else None\n",
    "\n",
    "# Helper function to fetch and convert table schema to JSON format\n",
    "def get_table_schema_json(catalog_name, schema_name, table_name):\n",
    "    query = f\"\"\"\n",
    "        SELECT column_name, data_type\n",
    "        FROM {catalog_name}.information_schema.columns \n",
    "        WHERE table_schema = '{schema_name}' \n",
    "          AND table_name = '{table_name}'\n",
    "    \"\"\"\n",
    "    table_schema = spark.sql(query)\n",
    "    return json.dumps({row['column_name']: row['data_type'] for row in table_schema.collect()})\n",
    "\n",
    "# Helper function to define schema registry structure\n",
    "def get_schema_registry_schema():\n",
    "    return StructType([\n",
    "        StructField(\"catalog_name\", StringType(), False),\n",
    "        StructField(\"schema_name\", StringType(), False),\n",
    "        StructField(\"table_name\", StringType(), False),\n",
    "        StructField(\"schema_version\", IntegerType(), False),\n",
    "        StructField(\"created_by\", StringType(), True),\n",
    "        StructField(\"modified_by\", StringType(), True),\n",
    "        StructField(\"modified_timestamp\", TimestampType(), True),\n",
    "        StructField(\"schema_json\", StringType(), False),\n",
    "        StructField(\"change_type\", StringType(), True),\n",
    "        StructField(\"column_name\", StringType(), True),\n",
    "        StructField(\"table_version\", IntegerType(), False),\n",
    "        StructField(\"table_version_timestamp\", TimestampType(), True),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "        StructField(\"check_timestamp\", TimestampType(), False),\n",
    "        StructField(\"schema_change_alert_status\", StringType(), True),\n",
    "        StructField(\"rollback_notification_status\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "# New function to prepare data for schema registry entry\n",
    "def prepare_schema_registry_data(catalog_name, schema_name, table_name, schema_version, created_by, schema_json, \n",
    "                                 latest_table_version, table_version_timestamp, schema_change_alert_status=None, \n",
    "                                 rollback_notification_status=None):\n",
    "    # Prepare the data dictionary\n",
    "    return [{\n",
    "        \"catalog_name\": catalog_name,\n",
    "        \"schema_name\": schema_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"schema_version\": schema_version,\n",
    "        \"created_by\": created_by,\n",
    "        \"modified_by\": None,\n",
    "        \"modified_timestamp\": None,\n",
    "        \"schema_json\": schema_json,\n",
    "        \"change_type\": None,\n",
    "        \"column_name\": None,\n",
    "        \"table_version\": latest_table_version,\n",
    "        \"table_version_timestamp\": table_version_timestamp,\n",
    "        \"status\": \"Active\",\n",
    "        \"check_timestamp\": datetime.utcnow(),\n",
    "        \"schema_change_alert_status\": schema_change_alert_status,\n",
    "        \"rollback_notification_status\": rollback_notification_status\n",
    "    }]\n",
    "\n",
    "# Main function to populate schema registry\n",
    "def populate_schema_registry(table_reference):\n",
    "    catalog_name, schema_name, table_name = table_reference.split('.')\n",
    "    \n",
    "    # Step 1: Check if the table already exists in schema_registry\n",
    "    schema_registry_check = check_schema_registry(catalog_name, schema_name, table_name)\n",
    "    \n",
    "    if schema_registry_check.count() > 0:\n",
    "        print(f\"Table {catalog_name}.{schema_name}.{table_name} is already present in schema_registry.\")\n",
    "        return  # Exit if the table already exists\n",
    "    \n",
    "    # Step 2: Fetch latest table version and timestamp from DESCRIBE HISTORY\n",
    "    latest_table_version, table_version_timestamp = get_latest_table_version(catalog_name, schema_name, table_name)\n",
    "    \n",
    "    # Step 3: Fetch created_by (table owner)\n",
    "    created_by = get_table_owner(catalog_name, schema_name, table_name)\n",
    "    \n",
    "    # Step 4: Fetch schema and convert to JSON\n",
    "    schema_json = get_table_schema_json(catalog_name, schema_name, table_name)\n",
    "    \n",
    "    # Step 5: Prepare data for schema registry entry\n",
    "    schema_version = 1  # Starting with version 1 for new tables\n",
    "    schema_registry_data = prepare_schema_registry_data(\n",
    "        catalog_name=catalog_name,\n",
    "        schema_name=schema_name,\n",
    "        table_name=table_name,\n",
    "        schema_version=schema_version,\n",
    "        created_by=created_by,\n",
    "        schema_json=schema_json,\n",
    "        latest_table_version=latest_table_version,\n",
    "        table_version_timestamp=table_version_timestamp\n",
    "    )\n",
    "    \n",
    "    # Step 6: Create DataFrame and insert into schema registry\n",
    "    schema_registry_schema = get_schema_registry_schema()\n",
    "    schema_registry_df = spark.createDataFrame(schema_registry_data, schema=schema_registry_schema)\n",
    "    schema_registry_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.default.schema_registry\")\n",
    "    print(f\"Table {catalog_name}.{schema_name}.{table_name} successfully added to schema_registry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b4facb-210f-4c32-aaf4-8d9b9ec4d57f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table ds_training_1.ds_bronze.book_xml_vol is already present in schema_registry.\nTable ds_training_1.ds_gold.book_author is already present in schema_registry.\nTable ds_training_1.ds_silver.book_parquet is already present in schema_registry.\nTable ds_training_1.ds_silver.book is already present in schema_registry.\n"
     ]
    }
   ],
   "source": [
    "def populate_for_all_tables(catalog_name):\n",
    "    # Fetch all schemas in the specified catalog\n",
    "    schemas = spark.sql(f\"\"\"\n",
    "        SELECT schema_name \n",
    "        FROM {catalog_name}.information_schema.schemata where schema_name <> 'information_schema'\n",
    "    \"\"\").collect()\n",
    "\n",
    "    # Iterate through the list of schemas\n",
    "    for row in schemas:\n",
    "        schema_name = row['schema_name']\n",
    "        \n",
    "        # Fetch all tables that we are monitoring in the current schema \n",
    "        tables = spark.sql(f\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM {catalog_name}.information_schema.tables \n",
    "            WHERE table_schema = '{schema_name}' and created_by='brindavivek.kotha@latentviewo365.onmicrosoft.com' and table_name like '%book%'\n",
    "        \"\"\").collect()\n",
    "\n",
    "        # Iterate through the list of tables and call the populate function\n",
    "        for table_row in tables:\n",
    "            table_name = table_row['table_name']\n",
    "            table_reference = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "            populate_schema_registry(table_reference)\n",
    "\n",
    "# Example call to populate for all tables in a specific catalog\n",
    "populate_for_all_tables(catalog_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38d891a-5e22-489d-990e-92f18b57630d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4482bb3-e0eb-4da2-b961-e8e48f97215d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_latest_check_timestamp(catalog_name, table_name):\n",
    "    \"\"\"Retrieve the latest check timestamp from the schema registry.\"\"\"\n",
    "    latest_timestamp_df = spark.sql(f\"\"\"\n",
    "        SELECT check_timestamp \n",
    "        FROM {catalog_name}.default.schema_registry \n",
    "        WHERE table_name = '{table_name}' \n",
    "        ORDER BY modified_timestamp DESC \n",
    "        LIMIT 1\n",
    "    \"\"\")\n",
    "    return latest_timestamp_df.collect()[0]['check_timestamp'] if latest_timestamp_df.count() > 0 else None\n",
    "\n",
    "def get_max_schema_version(catalog_name, table_name):\n",
    "    \"\"\"Get the maximum schema version from the schema registry.\"\"\"\n",
    "    max_version_df = spark.sql(f\"\"\"\n",
    "        SELECT MAX(schema_version) AS max_version \n",
    "        FROM {catalog_name}.default.schema_registry \n",
    "        WHERE table_name = '{table_name}'\n",
    "    \"\"\")\n",
    "    return max_version_df.collect()[0]['max_version'] if max_version_df.count() > 0 else None\n",
    "\n",
    "def fetch_created_by(catalog_name, schema_name, table_name):\n",
    "    \"\"\"Retrieve the 'created_by' information from information_schema.tables.\"\"\"\n",
    "    created_by_df = spark.sql(f\"\"\"\n",
    "        SELECT created_by \n",
    "        FROM {catalog_name}.information_schema.tables \n",
    "        WHERE table_schema = '{schema_name}' AND table_name = '{table_name}'\n",
    "    \"\"\")\n",
    "    return created_by_df.collect()[0]['created_by'] if created_by_df.count() > 0 else \"unknown\"\n",
    "\n",
    "def fetch_schema_json(catalog_name, schema_name, table_name):\n",
    "    \"\"\"Retrieve the schema as JSON from information_schema.columns.\"\"\"\n",
    "    schema_df = spark.sql(f\"\"\"\n",
    "        SELECT COLUMN_NAME, DATA_TYPE \n",
    "        FROM {catalog_name}.information_schema.columns \n",
    "        WHERE table_schema = '{schema_name}' AND table_name = '{table_name}'\n",
    "    \"\"\")\n",
    "    return {row['COLUMN_NAME']: row['DATA_TYPE'] for row in schema_df.collect()}\n",
    "\n",
    "def collect_operations_after_check_timestamp(describe_history_df, latest_check_timestamp):\n",
    "    \"\"\"Collect operations that occurred after the latest check timestamp.\"\"\"\n",
    "    operations_after_check_timestamp = []\n",
    "    \n",
    "    for row in describe_history_df.collect():\n",
    "        operation_timestamp = row['timestamp']\n",
    "        if latest_check_timestamp and operation_timestamp <= latest_check_timestamp:\n",
    "            continue  # Skip operations that occurred before or at the latest check timestamp\n",
    "\n",
    "        operation = row['operation']\n",
    "        modified_by = row['userName']\n",
    "        operation_parameters = row['operationParameters']\n",
    "\n",
    "        # Handle operation parameters\n",
    "        if isinstance(operation_parameters, str):\n",
    "            operation_params_dict = json.loads(operation_parameters)\n",
    "        else:\n",
    "            operation_params_dict = operation_parameters\n",
    "\n",
    "        drop_column_name = None\n",
    "        rename_column_name = None\n",
    "        column_names = []  # For storing names of added columns\n",
    "\n",
    "        # Process based on operation type\n",
    "        if operation == \"ADD COLUMNS\" and operation_params_dict is not None:\n",
    "            if 'columns' in operation_params_dict:\n",
    "                columns_list = json.loads(operation_params_dict['columns'])\n",
    "                column_names = [col['column']['name'] for col in columns_list]\n",
    "\n",
    "        elif operation == \"DROP COLUMNS\" and operation_params_dict is not None:\n",
    "            if 'columns' in operation_params_dict:\n",
    "                columns = json.loads(operation_params_dict['columns'])\n",
    "                drop_column_name = columns[0]  # Assuming only one column is dropped\n",
    "\n",
    "        elif operation == \"RENAME COLUMN\" and operation_params_dict is not None:\n",
    "            if 'oldColumnPath' in operation_params_dict and 'newColumnPath' in operation_params_dict:\n",
    "                rename_column_name = operation_params_dict['newColumnPath']  # Store the new column name\n",
    "\n",
    "        # Append operation details to the list\n",
    "        operations_after_check_timestamp.append({\n",
    "            \"operation\": operation,\n",
    "            \"modified_by\": modified_by,\n",
    "            \"add_column_names\": column_names,\n",
    "            \"drop_column_name\": drop_column_name,\n",
    "            \"rename_column_name\": rename_column_name,\n",
    "            \"timestamp\": operation_timestamp,\n",
    "            \"version\": row['version']\n",
    "        })\n",
    "    \n",
    "    return operations_after_check_timestamp\n",
    "def detect_data_type_changes(operations_after_check_timestamp):\n",
    "    # Step 5: Detect data type change based on the specific sequence of operations\n",
    "    operation_sequences = [\n",
    "        (\"RENAME COLUMN\", \"DROP COLUMNS\", \"UPDATE\", \"ADD COLUMNS\")\n",
    "    ]\n",
    "    data_type_changes = []  # List to store the detected changes\n",
    "\n",
    "    # Iterate over the operations\n",
    "    for index in range(len(operations_after_check_timestamp) - 3):\n",
    "        # Extract the last four operations from the current index\n",
    "        current_sequence = (\n",
    "            operations_after_check_timestamp[index]['operation'],\n",
    "            operations_after_check_timestamp[index + 1]['operation'],\n",
    "            operations_after_check_timestamp[index + 2]['operation'],\n",
    "            operations_after_check_timestamp[index + 3]['operation']\n",
    "        )\n",
    "        \n",
    "        # Check if the current sequence matches the defined operation sequence\n",
    "        if current_sequence == operation_sequences[0]:\n",
    "            rename_operation = operations_after_check_timestamp[index]\n",
    "            drop_operation = operations_after_check_timestamp[index + 1]\n",
    "            \n",
    "            # Check if the drop column name matches the rename column name\n",
    "            if drop_operation.get('drop_column_name') == rename_operation.get('rename_column_name'):\n",
    "                change_column_name = rename_operation.get('rename_column_name')\n",
    "                change_index = index + 3  # Mark the index of the change\n",
    "                \n",
    "                # Append the result to the list\n",
    "                data_type_changes.append({\n",
    "                    'change_column_name': change_column_name,\n",
    "                    'change_index': change_index\n",
    "                })\n",
    "\n",
    "    # Output the results\n",
    "    if data_type_changes:\n",
    "        print(\"Data type changes detected:\")\n",
    "        for change in data_type_changes:\n",
    "            print(f\"Column: '{change['change_column_name']}' at index {change['change_index']}\")\n",
    "        return data_type_changes\n",
    "    else:\n",
    "        print(\"No data type changes detected.\")\n",
    "        return []\n",
    "\n",
    "def define_schema():\n",
    "    \"\"\"Define the schema for the DataFrame.\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"catalog_name\", StringType(), True),\n",
    "        StructField(\"schema_name\", StringType(), True),\n",
    "        StructField(\"table_name\", StringType(), True),\n",
    "        StructField(\"schema_version\", IntegerType(), True),\n",
    "        StructField(\"created_by\", StringType(), True),\n",
    "        StructField(\"modified_by\", StringType(), True),\n",
    "        StructField(\"modified_timestamp\", TimestampType(), True),\n",
    "        StructField(\"schema_json\", StringType(), True),\n",
    "        StructField(\"change_type\", StringType(), True),\n",
    "        StructField(\"column_name\", StringType(), True),\n",
    "        StructField(\"table_version\", IntegerType(), True),\n",
    "        StructField(\"table_version_timestamp\", TimestampType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"check_timestamp\", TimestampType(), True),\n",
    "        StructField(\"schema_change_alert_status\", StringType(), True),  # New field added\n",
    "        StructField(\"rollback_notification_status\", StringType(), True)  # New field added\n",
    "    ])\n",
    "\n",
    "def create_schema_registry_entry(schema_registry_data, catalog_name, schema_name, table_name, schema_version, created_by, modified_by, modified_timestamp, schema_json, change_type, column_name, table_version, table_version_timestamp, status):\n",
    "    \"\"\"Append a new entry to the schema_registry_data.\"\"\"\n",
    "    schema_registry_data.append({\n",
    "        \"catalog_name\": catalog_name,\n",
    "        \"schema_name\": schema_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"schema_version\": schema_version,\n",
    "        \"created_by\": created_by,\n",
    "        \"modified_by\": modified_by,\n",
    "        \"modified_timestamp\": modified_timestamp,\n",
    "        \"schema_json\": schema_json,\n",
    "        \"change_type\": change_type,\n",
    "        \"column_name\": column_name,\n",
    "        \"table_version\": table_version,\n",
    "        \"table_version_timestamp\": table_version_timestamp,\n",
    "        \"status\": status,\n",
    "        \"check_timestamp\": datetime.utcnow(),\n",
    "        \"schema_change_alert_status\": \"Pending\",  # Default value\n",
    "        \"rollback_notification_status\": \"NA\"  # Default value\n",
    "    })\n",
    "\n",
    "\n",
    "def mark_existing_records_inactive(catalog_name, table_name):\n",
    "    \"\"\"Update all existing records for this table's status to 'Inactive'.\"\"\"\n",
    "    update_query = f\"\"\"\n",
    "        UPDATE {catalog_name}.default.schema_registry \n",
    "        SET status = 'Inactive' \n",
    "        WHERE table_name = '{table_name}' AND status = 'Active'\n",
    "    \"\"\"\n",
    "    spark.sql(update_query)\n",
    "\n",
    "def update_schema_registry_with_changes(table_reference):\n",
    "    catalog_name, schema_name, table_name = table_reference.split('.')\n",
    "\n",
    "    # Step 1: Get the latest check timestamp and maximum schema version\n",
    "    latest_check_timestamp = get_latest_check_timestamp(catalog_name, table_name)\n",
    "    max_version = get_max_schema_version(catalog_name, table_name)\n",
    "    new_version = (max_version + 1) if max_version is not None else 1\n",
    "\n",
    "    # Step 2: Describe history to collect operations\n",
    "    describe_history_df = spark.sql(f\"DESCRIBE HISTORY {catalog_name}.{schema_name}.{table_name}\")\n",
    "    operations_after_check_timestamp = collect_operations_after_check_timestamp(describe_history_df, latest_check_timestamp)\n",
    "\n",
    "    # Step to exit: Check if there are any new operations\n",
    "    if not operations_after_check_timestamp:\n",
    "        print(\"No new operations to process.\")\n",
    "        return \n",
    "    # Step 3: Prepare data for the schema registry\n",
    "    schema_registry_data = []\n",
    "\n",
    "    # Step 4: Get created_by from information_schema.tables\n",
    "    created_by = fetch_created_by(catalog_name, schema_name, table_name)\n",
    "\n",
    "    # Step 5: Retrieve schema from information_schema for the schema_json\n",
    "    schema_json = fetch_schema_json(catalog_name, schema_name, table_name)\n",
    "    \n",
    "    #Data type change detection\n",
    "    data_type_changes = detect_data_type_changes(operations_after_check_timestamp)\n",
    "\n",
    "    # Step 6: Initialize variables for tracking schema changes\n",
    "    schema_version = new_version - 1\n",
    "    skip_operations_until = -1  # To track how many iterations to skip after a data type change\n",
    "\n",
    "    # Step 7: Handle operations (data type changes and normal operations in sequence)\n",
    "    for i in range(len(operations_after_check_timestamp) - 1, -1, -1):  # Iterating backwards\n",
    "        operation_detail = operations_after_check_timestamp[i]\n",
    "\n",
    "        # Check if we should skip this iteration (after a data type change)\n",
    "        if i >= skip_operations_until and skip_operations_until != -1:\n",
    "            continue  # Skip the next operations after a data type change\n",
    "\n",
    "        # Check if the current index matches any data type change\n",
    "        #matching_change = next((change for change in detect_data_type_changes if change['change_index'] == i), None)\n",
    "        matching_change = next((change for change in data_type_changes if change['change_index'] == i), None)\n",
    "\n",
    "        if matching_change:\n",
    "            # Data type change detected, record as a single entry\n",
    "            schema_version += 1\n",
    "            create_schema_registry_entry(\n",
    "                schema_registry_data,\n",
    "                catalog_name,\n",
    "                schema_name,\n",
    "                table_name,\n",
    "                schema_version,\n",
    "                created_by,\n",
    "                operation_detail['modified_by'],\n",
    "                operation_detail['timestamp'],\n",
    "                json.dumps(schema_json) if (i - 3 == 0) else None,  # Record schema_json for the first change\n",
    "                \"DATA TYPE CHANGE\",\n",
    "                matching_change['change_column_name'],\n",
    "                int(operation_detail['version']),\n",
    "                operation_detail['timestamp'],\n",
    "                'Active' if (i - 3 == 0) else 'Inactive'# Status can be 'Active' or 'Inactive'\n",
    "          \n",
    "            )\n",
    "            # Update the skip_operations_until to ensure next operations are ignored\n",
    "            skip_operations_until = i - 3\n",
    "            continue\n",
    "        \n",
    "        # Record normal operations (addition, dropping, renaming)\n",
    "        if operation_detail['operation'] in [\"ADD COLUMNS\", \"DROP COLUMNS\", \"RENAME COLUMN\"]:\n",
    "            schema_version += 1\n",
    "            create_schema_registry_entry(\n",
    "                schema_registry_data,\n",
    "                catalog_name,\n",
    "                schema_name,\n",
    "                table_name,\n",
    "                schema_version,\n",
    "                created_by,\n",
    "                operation_detail['modified_by'],\n",
    "                operation_detail['timestamp'],\n",
    "                json.dumps(schema_json) if i == 0 else None,\n",
    "                operation_detail['operation'],\n",
    "                operation_detail.get('rename_column_name') or (operation_detail.get('add_column_names') or operation_detail.get('drop_column_name')),\n",
    "                int(operation_detail['version']),\n",
    "                operation_detail['timestamp'],\n",
    "                'Active' if i == 0 else 'Inactive'\n",
    "       \n",
    "\n",
    "            )\n",
    "\n",
    "    # Step 8: Mark existing records as inactive\n",
    "    mark_existing_records_inactive(catalog_name, table_name)\n",
    "\n",
    "    # Step 9: Write the new schema registry data back to the database\n",
    "    updated_schema_registry_df = spark.createDataFrame(schema_registry_data, schema=define_schema())\n",
    "    updated_schema_registry_df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{catalog_name}.default.schema_registry\")\n",
    "    display(updated_schema_registry_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b0c825-bcbe-4d0d-8ca3-33f1b7c110a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new operations to process.\nNo new operations to process.\nNo data type changes detected.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>catalog_name</th><th>schema_name</th><th>table_name</th><th>schema_version</th><th>created_by</th><th>modified_by</th><th>modified_timestamp</th><th>schema_json</th><th>change_type</th><th>column_name</th><th>table_version</th><th>table_version_timestamp</th><th>status</th><th>check_timestamp</th><th>schema_change_alert_status</th><th>rollback_notification_status</th></tr></thead><tbody><tr><td>ds_training_1</td><td>ds_silver</td><td>customer_silver_vishal</td><td>18</td><td>vishal.kokkula@latentviewo365.onmicrosoft.com</td><td>vishal.kokkula@latentviewo365.onmicrosoft.com</td><td>2024-10-23T10:40:30Z</td><td>null</td><td>ADD COLUMNS</td><td>['new_1']</td><td>26</td><td>2024-10-23T10:40:30Z</td><td>Inactive</td><td>2024-10-23T10:44:14.541861Z</td><td>Pending</td><td>NA</td></tr><tr><td>ds_training_1</td><td>ds_silver</td><td>customer_silver_vishal</td><td>19</td><td>vishal.kokkula@latentviewo365.onmicrosoft.com</td><td>vishal.kokkula@latentviewo365.onmicrosoft.com</td><td>2024-10-23T10:40:37Z</td><td>null</td><td>ADD COLUMNS</td><td>['new_2']</td><td>27</td><td>2024-10-23T10:40:37Z</td><td>Inactive</td><td>2024-10-23T10:44:14.541871Z</td><td>Pending</td><td>NA</td></tr><tr><td>ds_training_1</td><td>ds_silver</td><td>customer_silver_vishal</td><td>20</td><td>vishal.kokkula@latentviewo365.onmicrosoft.com</td><td>vishal.kokkula@latentviewo365.onmicrosoft.com</td><td>2024-10-23T10:41:26Z</td><td>{\"customer_id\": \"INT\", \"name\": \"STRING\", \"age\": \"INT\", \"gender\": \"STRING\", \"phone_number\": \"STRING\", \"email\": \"STRING\", \"account_id\": \"INT\", \"account_type\": \"STRING\", \"balance\": \"INT\", \"opened_date\": \"DATE\", \"status\": \"STRING\", \"business_date\": \"STRING\", \"test_col_3\": \"DATE\", \"test_column_18_10_24\": \"INT\", \"test_column_23_10\": \"INT\", \"new_1\": \"INT\", \"new_2\": \"INT\"}</td><td>RENAME COLUMN</td><td>test_col_3</td><td>28</td><td>2024-10-23T10:41:26Z</td><td>Active</td><td>2024-10-23T10:44:14.541913Z</td><td>Pending</td><td>NA</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ds_training_1",
         "ds_silver",
         "customer_silver_vishal",
         18,
         "vishal.kokkula@latentviewo365.onmicrosoft.com",
         "vishal.kokkula@latentviewo365.onmicrosoft.com",
         "2024-10-23T10:40:30Z",
         null,
         "ADD COLUMNS",
         "['new_1']",
         26,
         "2024-10-23T10:40:30Z",
         "Inactive",
         "2024-10-23T10:44:14.541861Z",
         "Pending",
         "NA"
        ],
        [
         "ds_training_1",
         "ds_silver",
         "customer_silver_vishal",
         19,
         "vishal.kokkula@latentviewo365.onmicrosoft.com",
         "vishal.kokkula@latentviewo365.onmicrosoft.com",
         "2024-10-23T10:40:37Z",
         null,
         "ADD COLUMNS",
         "['new_2']",
         27,
         "2024-10-23T10:40:37Z",
         "Inactive",
         "2024-10-23T10:44:14.541871Z",
         "Pending",
         "NA"
        ],
        [
         "ds_training_1",
         "ds_silver",
         "customer_silver_vishal",
         20,
         "vishal.kokkula@latentviewo365.onmicrosoft.com",
         "vishal.kokkula@latentviewo365.onmicrosoft.com",
         "2024-10-23T10:41:26Z",
         "{\"customer_id\": \"INT\", \"name\": \"STRING\", \"age\": \"INT\", \"gender\": \"STRING\", \"phone_number\": \"STRING\", \"email\": \"STRING\", \"account_id\": \"INT\", \"account_type\": \"STRING\", \"balance\": \"INT\", \"opened_date\": \"DATE\", \"status\": \"STRING\", \"business_date\": \"STRING\", \"test_col_3\": \"DATE\", \"test_column_18_10_24\": \"INT\", \"test_column_23_10\": \"INT\", \"new_1\": \"INT\", \"new_2\": \"INT\"}",
         "RENAME COLUMN",
         "test_col_3",
         28,
         "2024-10-23T10:41:26Z",
         "Active",
         "2024-10-23T10:44:14.541913Z",
         "Pending",
         "NA"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "catalog_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "schema_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "schema_version",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "created_by",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "modified_by",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "modified_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "schema_json",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "change_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "column_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_version",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "table_version_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "check_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "schema_change_alert_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rollback_notification_status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new operations to process.\nNo new operations to process.\nNo new operations to process.\nNo new operations to process.\nNo new operations to process.\nNo new operations to process.\n"
     ]
    }
   ],
   "source": [
    "def update_all_tables_in_schema_registry(catalog_name):\n",
    "    # Step 1: Get distinct schema and table names from the schema registry\n",
    "    distinct_tables_df = spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT schema_name, table_name \n",
    "        FROM {catalog_name}.default.schema_registry\n",
    "    \"\"\")\n",
    "\n",
    "    # Step 2: Loop through each distinct table and call update_schema_registry_with_changes\n",
    "    for row in distinct_tables_df.collect():\n",
    "        schema_name = row['schema_name']\n",
    "        table_name = row['table_name']\n",
    "        \n",
    "        # Prepare the table reference in the required format\n",
    "        table_reference = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "        \n",
    "        # Call the update function for each table\n",
    "        update_schema_registry_with_changes(table_reference)\n",
    "\n",
    "# Example call for all tables in the catalog 'ds_training_1'\n",
    "update_all_tables_in_schema_registry(catalog_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1184659034704069,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Populating_Schema_Registry",
   "widgets": {
    "catalog_name": {
     "currentValue": "ds_training_1",
     "nuid": "74448a32-c79d-42db-a273-a6b3fcde4570",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "catalog_name",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "catalog_name",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
